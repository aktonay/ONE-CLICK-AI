{# LLM Fine-tuning #}
"""
LLM fine-tuning with LoRA / QLoRA using PEFT + Transformers.
"""

import logging
from typing import Any, Dict, Optional
from pathlib import Path

logger = logging.getLogger(__name__)


class LLMFineTuner:
    """Fine-tune LLMs with LoRA/QLoRA (Parameter-Efficient Fine-Tuning)."""

    def __init__(
        self,
        base_model: str = "meta-llama/Llama-3.2-3B",
        output_dir: str = "models/fine_tuned",
        use_qlora: bool = True,
        lora_r: int = 16,
        lora_alpha: int = 32,
        lora_dropout: float = 0.05,
    ):
        self.base_model = base_model
        self.output_dir = output_dir
        self.use_qlora = use_qlora
        self.lora_r = lora_r
        self.lora_alpha = lora_alpha
        self.lora_dropout = lora_dropout

    def prepare_model(self):
        """Load base model with quantization config."""
        from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
        from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
        import torch

        bnb_config = None
        if self.use_qlora:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
            )

        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model)
        self.tokenizer.pad_token = self.tokenizer.eos_token

        self.model = AutoModelForCausalLM.from_pretrained(
            self.base_model,
            quantization_config=bnb_config,
            device_map="auto",
            torch_dtype=torch.bfloat16,
        )

        if self.use_qlora:
            self.model = prepare_model_for_kbit_training(self.model)

        lora_config = LoraConfig(
            r=self.lora_r,
            lora_alpha=self.lora_alpha,
            lora_dropout=self.lora_dropout,
            bias="none",
            task_type="CAUSAL_LM",
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        )

        self.model = get_peft_model(self.model, lora_config)
        trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total = sum(p.numel() for p in self.model.parameters())
        logger.info(f"Trainable params: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)")
        return self.model

    def train(self, dataset, epochs: int = 3, batch_size: int = 4, lr: float = 2e-4):
        """Run fine-tuning."""
        from transformers import TrainingArguments
        from trl import SFTTrainer

        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=epochs,
            per_device_train_batch_size=batch_size,
            learning_rate=lr,
            bf16=True,
            logging_steps=10,
            save_strategy="epoch",
            gradient_accumulation_steps=4,
            warmup_ratio=0.1,
            optim="paged_adamw_8bit" if self.use_qlora else "adamw_torch",
        )

        trainer = SFTTrainer(
            model=self.model,
            tokenizer=self.tokenizer,
            args=training_args,
            train_dataset=dataset,
            max_seq_length=2048,
        )

        trainer.train()
        trainer.save_model(self.output_dir)
        self.tokenizer.save_pretrained(self.output_dir)
        logger.info(f"Fine-tuned model saved to {self.output_dir}")

    def merge_and_export(self, export_path: str = "models/merged"):
        """Merge LoRA weights back into the base model for deployment."""
        from peft import PeftModel
        merged = self.model.merge_and_unload()
        merged.save_pretrained(export_path)
        self.tokenizer.save_pretrained(export_path)
        logger.info(f"Merged model exported to {export_path}")
