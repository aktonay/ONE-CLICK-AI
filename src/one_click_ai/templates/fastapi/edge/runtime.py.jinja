{# Edge AI Runtime #}
"""
Edge inference runtime â€” ONNX Runtime, TensorRT, CPU/GPU inference.
"""

import logging
import time
from typing import Any, Dict, List, Optional
import numpy as np

logger = logging.getLogger(__name__)


class EdgeRuntime:
    """Unified edge inference runtime."""

    def __init__(self, model_path: str, runtime: str = "onnx", device: str = "cpu"):
        self.model_path = model_path
        self.runtime = runtime
        self.device = device
        self._session = None
        self._load()

    def _load(self):
{% if onnx %}
        if self.runtime == "onnx":
            import onnxruntime as ort
            providers = ["CUDAExecutionProvider", "CPUExecutionProvider"] if self.device == "cuda" else ["CPUExecutionProvider"]
            self._session = ort.InferenceSession(self.model_path, providers=providers)
            logger.info(f"ONNX Runtime session created: {self.model_path} [{self._session.get_providers()}]")
            return
{% endif %}
{% if tensorrt %}
        if self.runtime == "tensorrt":
            logger.info(f"TensorRT engine loaded: {self.model_path}")
            return
{% endif %}
        raise ValueError(f"Unsupported runtime: {self.runtime}")

    def predict(self, input_data: np.ndarray) -> Dict[str, Any]:
        """Run inference."""
        start = time.perf_counter()

{% if onnx %}
        if self.runtime == "onnx":
            input_name = self._session.get_inputs()[0].name
            outputs = self._session.run(None, {input_name: input_data.astype(np.float32)})
            latency = (time.perf_counter() - start) * 1000
            return {
                "outputs": [o.tolist() for o in outputs],
                "latency_ms": latency,
                "runtime": "onnx",
            }
{% endif %}
        return {"error": "Runtime not available"}

    def benchmark(self, input_data: np.ndarray, iterations: int = 100) -> Dict[str, float]:
        """Benchmark inference speed."""
        latencies = []
        for _ in range(iterations):
            start = time.perf_counter()
            self.predict(input_data)
            latencies.append((time.perf_counter() - start) * 1000)

        return {
            "mean_ms": float(np.mean(latencies)),
            "p50_ms": float(np.percentile(latencies, 50)),
            "p95_ms": float(np.percentile(latencies, 95)),
            "p99_ms": float(np.percentile(latencies, 99)),
            "min_ms": float(np.min(latencies)),
            "max_ms": float(np.max(latencies)),
            "throughput_fps": float(1000 / np.mean(latencies)),
            "iterations": iterations,
        }

    def get_model_info(self) -> Dict[str, Any]:
        """Get model metadata."""
{% if onnx %}
        if self.runtime == "onnx":
            inputs = self._session.get_inputs()
            outputs = self._session.get_outputs()
            return {
                "inputs": [{"name": i.name, "shape": i.shape, "type": i.type} for i in inputs],
                "outputs": [{"name": o.name, "shape": o.shape, "type": o.type} for o in outputs],
                "providers": self._session.get_providers(),
            }
{% endif %}
        return {}
