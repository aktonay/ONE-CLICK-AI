{# Edge AI Model Converter #}
"""
Model conversion — PyTorch/TF → ONNX → TensorRT.
"""

import logging
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

logger = logging.getLogger(__name__)


class ModelConverter:
    """Convert models between frameworks for edge deployment."""

{% if pytorch %}
    @staticmethod
    def pytorch_to_onnx(
        model,
        input_shape: Tuple[int, ...] = (1, 3, 640, 640),
        output_path: str = "models/edge/model.onnx",
        opset_version: int = 17,
        dynamic_axes: Dict = None,
    ) -> str:
        """Export PyTorch model to ONNX format."""
        import torch

        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        model.eval()
        dummy_input = torch.randn(*input_shape)

        if dynamic_axes is None:
            dynamic_axes = {"input": {0: "batch_size"}, "output": {0: "batch_size"}}

        torch.onnx.export(
            model,
            dummy_input,
            output_path,
            opset_version=opset_version,
            input_names=["input"],
            output_names=["output"],
            dynamic_axes=dynamic_axes,
        )
        logger.info(f"Exported PyTorch → ONNX: {output_path}")
        return output_path
{% endif %}

{% if onnx %}
    @staticmethod
    def validate_onnx(model_path: str) -> Dict[str, Any]:
        """Validate an ONNX model."""
        import onnx
        model = onnx.load(model_path)
        onnx.checker.check_model(model)
        graph = model.graph
        return {
            "valid": True,
            "inputs": [i.name for i in graph.input],
            "outputs": [o.name for o in graph.output],
            "nodes": len(graph.node),
            "opset": model.opset_import[0].version,
        }
{% endif %}

{% if tensorrt %}
    @staticmethod
    def onnx_to_tensorrt(
        onnx_path: str,
        output_path: str = "models/edge/model.trt",
        fp16: bool = True,
        max_batch_size: int = 8,
    ) -> str:
        """Convert ONNX model to TensorRT engine."""
        try:
            import tensorrt as trt

            TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
            builder = trt.Builder(TRT_LOGGER)
            network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
            parser = trt.OnnxParser(network, TRT_LOGGER)

            with open(onnx_path, "rb") as f:
                if not parser.parse(f.read()):
                    for i in range(parser.num_errors):
                        logger.error(f"TensorRT parse error: {parser.get_error(i)}")
                    raise RuntimeError("TensorRT ONNX parsing failed")

            config = builder.create_builder_config()
            config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)

            if fp16:
                config.set_flag(trt.BuilderFlag.FP16)

            engine = builder.build_serialized_network(network, config)
            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, "wb") as f:
                f.write(engine)

            logger.info(f"Exported ONNX → TensorRT: {output_path}")
            return output_path
        except ImportError:
            logger.error("tensorrt not installed")
            raise
{% endif %}
